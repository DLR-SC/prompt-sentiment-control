{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt for Generating LLM-based Examples (Few-Shot Prompting)\n",
    "\n",
    "To generate LLM-based examples, we utilized a locally-hosted `LLaMa-2-7B-chat-hf` model, which was prompted with all 80 queries. For experimental consistency and to ensure direct comparability with the human-generated examples, we only included LLM-generated responses that corresponded to the same queries used in the human approach. The complete set of responses used in our experiments is available in the `few_shot.ipynb` notebook for reference and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "all_factual_queries = util.load_file('Queries/factual-queries.txt')\n",
    "factual_queries = all_factual_queries[10:50]\n",
    "\n",
    "all_subjective_queries = util.load_file('Queries/subjective-queries.txt')\n",
    "subjective_queries = all_subjective_queries[10:50]\n",
    "\n",
    "print(F\"{len(factual_queries)} factual queries have been loaded.\")\n",
    "print(F\"{len(subjective_queries)} subjective queries have been loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def chat(cl, ml=1000):\n",
    "    inputs = llama_tokenizer(cl, return_tensors=\"pt\")\n",
    "    generate_ids = llama_model.generate(inputs.input_ids, max_length=ml).to(DEVICE)\n",
    "\n",
    "    cl_new = llama_tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "    return(cl_new)\n",
    "\n",
    "model_path = \"ADD MODEL PATH\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "llama_tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "llama_model = transformers.AutoModelForCausalLM.from_pretrained(model_path).to(DEVICE)\n",
    "llama_model.to(DEVICE)\n",
    "\n",
    "emotions = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
    "persona = \"You are a helpful and honest assistant, skilled in writing emotional texts.\"\n",
    "\n",
    "\n",
    "########################################## FACTUAL\n",
    "\n",
    "responses = []\n",
    "emotion_list = []\n",
    "queries = []\n",
    "\n",
    "for query in factual_queries:\n",
    "    for emotion in emotions:\n",
    "        print(\"Started generating responses.\")\n",
    "\n",
    "        chat_log = F\"\"\"\n",
    "            [SYS]{persona}[/SYS] \n",
    "            [INST]\n",
    "                ###Instruction###: Answer the following question in the style of the given emotion.\n",
    "\n",
    "                ###Question###: {query} \n",
    "\n",
    "                ###Emotion###: {emotion}\n",
    "            [\\INST]\n",
    "            \"\"\"\n",
    "                \n",
    "        new_chat_log = chat(chat_log)\n",
    "        response = new_chat_log.replace(chat_log, \"\")\n",
    "        cleaned_response = re.sub(r'\\n', '', response)\n",
    "\n",
    "        responses.append(cleaned_response)\n",
    "        emotion_list.append(emotion)\n",
    "        queries.append(query)\n",
    "\n",
    "output_path = \"responses-llama_factual\"\n",
    "util.save_dataframe_files (responses, emotion_list, queries, output_path)\n",
    "\n",
    "########################################## SUBJECTIVE\n",
    "\n",
    "responses = []\n",
    "emotion_list = []\n",
    "queries = []\n",
    "\n",
    "for query in subjective_queries:\n",
    "    for emotion in emotions:\n",
    "        print(\"Started generating responses.\")\n",
    "\n",
    "        chat_log = F\"\"\"\n",
    "            [SYS]{persona}[/SYS] \n",
    "            [INST]\n",
    "                ###Instruction###: Write a text of 100 words based on the following input in the style of the given emotion.\n",
    "\n",
    "                ###Question###: {query} \n",
    "\n",
    "                ###Emotion###: {emotion}\n",
    "            [\\INST]\n",
    "            \"\"\"\n",
    "                \n",
    "        new_chat_log = chat(chat_log)\n",
    "        response = new_chat_log.replace(chat_log, \"\")\n",
    "        cleaned_response = re.sub(r'\\n', '', response)\n",
    "\n",
    "        responses.append(cleaned_response)\n",
    "        emotion_list.append(emotion)\n",
    "        queries.append(query)\n",
    "\n",
    "output_path = \"responses-llama_subjective\"\n",
    "util.save_dataframe_files (responses, emotion_list, queries, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt for Generating Reasoning Texts (CoT Prompting)\n",
    "\n",
    "We provide the original prompt used for generating the reasoning texts with GPT-3.5-turbo. However, we also added the reasoning texts used in our experiments in the folder cot_examples (auto_factual_reasoning and auto_subjective_reasoning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_factual_examples = pd.read_pickle(\"cot_examples/factual_examples\")\n",
    "df_subjective_examples = pd.read_pickle(\"cot_examples/subjective_examples\")\n",
    "\n",
    "output_path = \"cot_exmamples/reasoning_texts\"\n",
    "\n",
    "df_examples = pd.concat([df_factual_examples, df_subjective_examples], axis=0)\n",
    "\n",
    "responses = []\n",
    "\n",
    "for _, row in df_examples:\n",
    "  prompt = F\"Explain how the emotion {row['label']} is expressed in the following text. {row['text']}\"\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    seed = 16,\n",
    "    temperature = 0.0\n",
    "  )\n",
    "\n",
    "  responses.append(completion.choices[0].message.content)\n",
    "\n",
    "pd.to_pickle(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-sent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
